import os
import torch
from nodes import MAX_RESOLUTION
import torchvision.transforms.v2 as T
from PIL import Image, ImageDraw, ImageFont, ImageColor, ImageFilter
from comfy.utils import ProgressBar
import numpy as np

# å°è¯•è‡ªåŠ¨å®šä½å­—ä½“ç›®å½•
FONTS_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "font")
if not os.path.exists(FONTS_DIR):
    try:
        os.makedirs(FONTS_DIR, exist_ok=True)
    except:
        pass

class YC_SubtitleNode:
    """
    å­—å¹•åºåˆ—èŠ‚ç‚¹ (é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆ)
    
    ä¼˜åŒ–ç‚¹ï¼š
    1. å¼•å…¥ç¼“å­˜æœºåˆ¶ï¼šç›¸åŒçš„å­—å¹•æ®µè½åªæ¸²æŸ“ä¸€æ¬¡ã€‚
    2. ç§»é™¤é€å¸§ PIL è½¬æ¢ï¼šç›´æ¥åœ¨ Tensor å±‚é¢è¿›è¡Œå›¾åƒåˆæˆã€‚
    3. æå¤§æå‡å¤„ç†é€Ÿåº¦å¹¶é™ä½å†…å­˜æŠ–åŠ¨ã€‚
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        fonts = []
        if os.path.exists(FONTS_DIR):
            fonts = sorted([f for f in os.listdir(FONTS_DIR) if f.endswith('.ttf') or f.endswith('.otf')])
        if not fonts:
            fonts = ["default"]
        
        return {
            "required": {
                "images": ("IMAGE",), # [Batch, H, W, C]
                "subtitle_text": ("STRING", {
                    "multiline": True,
                    "default": "ç¬¬ä¸€æ®µå­—å¹•(30å¸§)||ç¬¬äºŒæ®µå­—å¹•(60å¸§)",
                }),
                "frame_durations": ("STRING", {
                    "multiline": True, 
                    "default": "30|15|60",
                }),
                "delimiter": ("STRING", {"default": "|"}),
                "font": (fonts, {"default": fonts[0] if fonts else "default"}),
                "font_size": ("INT", {"default": 48, "min": 8, "max": 500}),
                "text_color": ("STRING", {"default": "#FFFFFF"}),
                "stroke_width": ("INT", {"default": 0, "min": 0, "max": 20}),
                "stroke_color": ("STRING", {"default": "#000000"}),
                "background_color": ("STRING", {"default": "#00000000"}),
                "horizontal_align": (["left", "center", "right"], {"default": "center"}),
                "vertical_align": (["top", "center", "bottom"], {"default": "bottom"}),
                "offset_x": ("INT", {"default": 0, "min": -MAX_RESOLUTION, "max": MAX_RESOLUTION}),
                "offset_y": ("INT", {"default": -50, "min": -MAX_RESOLUTION, "max": MAX_RESOLUTION}),
                "shadow_enabled": (["disabled", "enabled"], {"default": "enabled"}),
                "shadow_distance": ("INT", {"default": 2, "min": 0, "max": 50}),
                "shadow_blur": ("INT", {"default": 3, "min": 0, "max": 50}),
                "shadow_expand": ("INT", {"default": 0, "min": 0, "max": 30}),
                "shadow_color": ("STRING", {"default": "#000000"}),
            },
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    FUNCTION = "execute"
    CATEGORY = "YC_VideoCutHelper/Subtitle"
    
    def parse_subtitle_text(self, text, delimiter):
        if not text:
            return []
        segments = text.split(delimiter)
        return [s.strip() for s in segments]
    
    def parse_frame_durations(self, frame_str, delimiter, count_needed):
        if not frame_str:
            return [30] * count_needed
        try:
            durations = [int(x.strip()) for x in frame_str.split(delimiter) if x.strip().isdigit()]
        except ValueError:
            print("[YC_Subtitle] å¸§æ•°æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨äº†é»˜è®¤å€¼30")
            durations = []
        if not durations:
            return [30] * count_needed
        if len(durations) < count_needed:
            durations.extend([durations[-1]] * (count_needed - len(durations)))
        return durations

    def get_font(self, font_name, font_size):
        if font_name != "default" and os.path.exists(FONTS_DIR):
            font_path = os.path.join(FONTS_DIR, font_name)
            if os.path.exists(font_path):
                try:
                    return ImageFont.truetype(font_path, font_size)
                except:
                    pass
        try:
            return ImageFont.load_default()
        except:
            return ImageFont.load_default()

    def get_text_size(self, text, font, stroke_width=0):
        try:
            left, top, right, bottom = font.getbbox(text, stroke_width=stroke_width)
            return right - left, bottom - top
        except TypeError:
            try:
                left, top, right, bottom = font.getbbox(text)
                return (right - left) + stroke_width * 2, (bottom - top) + stroke_width * 2
            except:
                return font.getsize(text)
        except:
            return font.getsize(text)

    # æ ¸å¿ƒä¼˜åŒ–ï¼šåªç”Ÿæˆä¸€å¼ åŒ…å«å­—å¹•çš„é€æ˜ Tensorï¼Œä¸åˆæˆåˆ°åŸå›¾
    def create_subtitle_mask(self, width, height, text, font, text_color_str, stroke_width, stroke_color_str, background_color_str,
                           horizontal_align, vertical_align, offset_x, offset_y,
                           shadow_enabled, shadow_distance, shadow_blur, shadow_expand, shadow_color_str):
        
        # 1. åŸºç¡€è®¾ç½®
        lines = text.split("\n")
        try:
            ascent, descent = font.getmetrics()
            line_spacing = ascent + descent
        except:
            line_spacing = font.size * 1.2
        line_spacing += stroke_width
            
        line_dims = []
        for line in lines:
            if not line.strip():
                line_dims.append((0, 0))
            else:
                line_dims.append(self.get_text_size(line, font, stroke_width))
        
        content_height = len(lines) * line_spacing

        if vertical_align == "top":
            start_y = offset_y
        elif vertical_align == "center":
            start_y = (height - content_height) / 2 + offset_y
        else:  # bottom
            start_y = height - content_height + offset_y
            
        def parse_color(c_str, default_alpha=255):
            try:
                if c_str.startswith('#'):
                    c_str = c_str.strip()
                    if len(c_str) == 7: return ImageColor.getrgb(c_str) + (default_alpha,)
                    if len(c_str) == 9: return ImageColor.getrgb(c_str[:7]) + (int(c_str[7:9], 16),)
                return ImageColor.getrgb(c_str) + (default_alpha,)
            except:
                return (255, 255, 255, default_alpha)

        text_rgba = parse_color(text_color_str, 255)
        stroke_rgba = parse_color(stroke_color_str, 255)
        bg_rgba = parse_color(background_color_str, 0)
        shadow_rgba = parse_color(shadow_color_str, 255)

        # 2. åˆ›å»º PIL å›¾å±‚
        layer = Image.new('RGBA', (width, height), color=bg_rgba)
        
        # 3. ç»˜åˆ¶é˜´å½±
        if shadow_enabled == "enabled" and shadow_distance > 0:
            shadow_layer = Image.new('RGBA', (width, height), (0,0,0,0))
            shadow_draw = ImageDraw.Draw(shadow_layer)
            curr_y = start_y
            shadow_stroke_width = stroke_width + shadow_expand
            for i, line in enumerate(lines):
                if line.strip():
                    w, h = line_dims[i]
                    if horizontal_align == "left": x = offset_x
                    elif horizontal_align == "center": x = (width - w) / 2 + offset_x
                    else: x = width - w + offset_x
                    
                    shadow_draw.text((x + shadow_distance, curr_y + shadow_distance), line, font=font, 
                                   fill=shadow_rgba, stroke_width=shadow_stroke_width, stroke_fill=shadow_rgba)
                curr_y += line_spacing
            if shadow_blur > 0:
                shadow_layer = shadow_layer.filter(ImageFilter.GaussianBlur(shadow_blur))
            layer = Image.alpha_composite(layer, shadow_layer)

        # 4. ç»˜åˆ¶æ­£æ–‡
        draw = ImageDraw.Draw(layer)
        curr_y = start_y
        for i, line in enumerate(lines):
            if line.strip():
                w, h = line_dims[i]
                if horizontal_align == "left": x = offset_x
                elif horizontal_align == "center": x = (width - w) / 2 + offset_x
                else: x = width - w + offset_x
                
                draw.text((x, curr_y), line, font=font, fill=text_rgba, 
                          stroke_width=stroke_width, stroke_fill=stroke_rgba)
            curr_y += line_spacing
            
        # 5. å…³é”®ä¼˜åŒ–ï¼šè½¬æ¢ä¸º Tensorï¼Œå½’ä¸€åŒ–åˆ° [0, 1]
        # PIL (RGBA) -> Numpy -> Tensor [H, W, 4]
        mask_np = np.array(layer).astype(np.float32) / 255.0
        mask_tensor = torch.from_numpy(mask_np) # [H, W, 4]
        
        return mask_tensor

    def execute(self, images, subtitle_text, frame_durations, delimiter,
                font, font_size, text_color, stroke_width, stroke_color, background_color,
                horizontal_align, vertical_align, offset_x, offset_y,
                shadow_enabled, shadow_distance, shadow_blur, shadow_expand, shadow_color):
        
        # 1. è§£æå‚æ•°
        segments = self.parse_subtitle_text(subtitle_text, delimiter)
        if not segments:
            return (images,)
        durations = self.parse_frame_durations(frame_durations, delimiter, len(segments))
        
        batch_size, height, width, channels = images.shape
        font_obj = self.get_font(font, font_size)
        
        # 2. é¢„æ¸²æŸ“ç¼“å­˜ (Cache Pre-rendering)
        # æ‰¾å‡ºæ‰€æœ‰ä¸é‡å¤çš„éç©ºå­—å¹•æ–‡æœ¬ï¼Œå…ˆæ¸²æŸ“æˆé®ç½© Tensor
        unique_texts = set([s for s in segments if s and s.strip()])
        text_cache = {}
        
        print(f"[YC_Subtitle] æ­£åœ¨é¢„æ¸²æŸ“ {len(unique_texts)} ä¸ªå”¯ä¸€çš„å­—å¹•é®ç½©...")
        
        for text in unique_texts:
            mask_tensor = self.create_subtitle_mask(
                width, height, text, font_obj, 
                text_color, stroke_width, stroke_color, background_color,
                horizontal_align, vertical_align, offset_x, offset_y,
                shadow_enabled, shadow_distance, shadow_blur, shadow_expand, shadow_color
            )
            # ç¡®ä¿ mask åœ¨ä¸ images ç›¸åŒçš„è®¾å¤‡ä¸Š (CPU/GPU)
            if images.device != mask_tensor.device:
                mask_tensor = mask_tensor.to(images.device)
            text_cache[text] = mask_tensor

        print(f"[YC_Subtitle] é¢„æ¸²æŸ“å®Œæˆã€‚å¼€å§‹åˆæˆè§†é¢‘å¸§...")

        # 3. å»ºç«‹å¸§ç´¢å¼•æ˜ å°„
        # result_images ç›´æ¥å…‹éš†è¾“å…¥ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼ˆComfyUIåŸåˆ™ï¼‰
        result_images = images.clone() 
        
        current_frame_idx = 0
        pbar = ProgressBar(len(segments))
        
        for i, seg_text in enumerate(segments):
            duration = durations[i]
            start_frame = current_frame_idx
            end_frame = min(current_frame_idx + duration, batch_size)
            
            # å¦‚æœè¿™å‡ å¸§æœ‰å­—å¹•ï¼Œä¸”å­—å¹•ä¸ä¸ºç©º
            if seg_text and seg_text.strip() and start_frame < batch_size:
                # è·å–ç¼“å­˜çš„é®ç½©: [H, W, 4]
                mask = text_cache[seg_text]
                
                # åˆ†ç¦» RGB å’Œ Alpha
                # overlay_rgb: [H, W, 3]
                # overlay_alpha: [H, W, 1]
                overlay_rgb = mask[:, :, :3]
                overlay_alpha = mask[:, :, 3:4]
                
                # 4. æ‰¹é‡å¼ é‡åˆæˆ (Vectorized Compositing)
                # æˆ‘ä»¬ä¸€æ¬¡æ€§å¤„ç†è¿™ä¸€æ®µçš„æ‰€æœ‰å¸§ [Start:End, H, W, C]
                # å…¬å¼: Target = Source * (1 - Alpha) + Overlay * Alpha
                
                # åˆ©ç”¨å¹¿æ’­æœºåˆ¶ï¼š
                # frame_slice: [N, H, W, 3]
                # overlay_alpha: [H, W, 1] -> å¹¿æ’­ä¸º [N, H, W, 1] -> [N, H, W, 3]
                # overlay_rgb: [H, W, 3] -> å¹¿æ’­ä¸º [N, H, W, 3]
                
                frame_slice = result_images[start_frame:end_frame]
                
                # æ‰§è¡Œåˆæˆè¿ç®—
                # æ³¨æ„ï¼šinplace æ“ä½œæ¯”åˆ›å»ºæ–° tensor æ›´çœæ˜¾å­˜
                # frame_slice * (1 - alpha)
                frame_slice.mul_(1.0 - overlay_alpha) 
                # + overlay * alpha
                frame_slice.add_(overlay_rgb * overlay_alpha)
                
            current_frame_idx = end_frame
            pbar.update(1)
            
            if current_frame_idx >= batch_size:
                break
                
        return (result_images,)

NODE_CLASS_MAPPINGS = {
    "YC_Subtitle": YC_SubtitleNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "YC_Subtitle": "ğŸ¬ YC Subtitle (Optimized)",
}